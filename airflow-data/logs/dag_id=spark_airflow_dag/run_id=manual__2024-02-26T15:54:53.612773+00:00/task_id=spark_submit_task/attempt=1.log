[2024-02-26T15:55:01.703+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_airflow_dag.spark_submit_task manual__2024-02-26T15:54:53.612773+00:00 [queued]>
[2024-02-26T15:55:01.713+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_airflow_dag.spark_submit_task manual__2024-02-26T15:54:53.612773+00:00 [queued]>
[2024-02-26T15:55:01.714+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2024-02-26T15:55:01.751+0000] {taskinstance.py:1327} INFO - Executing <Task(SparkSubmitOperator): spark_submit_task> on 2024-02-26 15:54:53.612773+00:00
[2024-02-26T15:55:01.762+0000] {standard_task_runner.py:57} INFO - Started process 11093 to run task
[2024-02-26T15:55:01.782+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'spark_airflow_dag', 'spark_submit_task', 'manual__2024-02-26T15:54:53.612773+00:00', '--job-id', '18', '--raw', '--subdir', 'DAGS_FOLDER/spark_etl_dag.py', '--cfg-path', '/tmp/tmp517cb833']
[2024-02-26T15:55:01.785+0000] {standard_task_runner.py:85} INFO - Job 18: Subtask spark_submit_task
[2024-02-26T15:55:01.822+0000] {logging_mixin.py:150} WARNING - /home/airflow/.local/lib/python3.7/site-packages/airflow/settings.py:195 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-02-26T15:55:01.933+0000] {task_command.py:410} INFO - Running <TaskInstance: spark_airflow_dag.spark_submit_task manual__2024-02-26T15:54:53.612773+00:00 [running]> on host 1db92e94aa46
[2024-02-26T15:55:02.097+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='spark_airflow_dag' AIRFLOW_CTX_TASK_ID='spark_submit_task' AIRFLOW_CTX_EXECUTION_DATE='2024-02-26T15:54:53.612773+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-02-26T15:54:53.612773+00:00'
[2024-02-26T15:55:02.106+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2024-02-26T15:55:02.116+0000] {spark_submit.py:339} INFO - Spark-Submit cmd: spark-submit --master local[*] --name arrow-spark /opt/airflow/dags/spark_etl.py
[2024-02-26T15:55:02.334+0000] {spark_submit.py:490} INFO - /home/airflow/.local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-02-26T15:55:12.509+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:12 INFO SparkContext: Running Spark version 3.4.2
[2024-02-26T15:55:12.685+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-02-26T15:55:12.798+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:12 INFO ResourceUtils: ==============================================================
[2024-02-26T15:55:12.799+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:12 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-02-26T15:55:12.800+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:12 INFO ResourceUtils: ==============================================================
[2024-02-26T15:55:12.800+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:12 INFO SparkContext: Submitted application: DataExtraction
[2024-02-26T15:55:12.821+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-02-26T15:55:12.834+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:12 INFO ResourceProfile: Limiting resource is cpu
[2024-02-26T15:55:12.835+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-02-26T15:55:12.891+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:12 INFO SecurityManager: Changing view acls to: airflow
[2024-02-26T15:55:12.891+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:12 INFO SecurityManager: Changing modify acls to: airflow
[2024-02-26T15:55:12.892+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:12 INFO SecurityManager: Changing view acls groups to:
[2024-02-26T15:55:12.893+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:12 INFO SecurityManager: Changing modify acls groups to:
[2024-02-26T15:55:12.893+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY
[2024-02-26T15:55:13.086+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO Utils: Successfully started service 'sparkDriver' on port 40361.
[2024-02-26T15:55:13.139+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO SparkEnv: Registering MapOutputTracker
[2024-02-26T15:55:13.166+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO SparkEnv: Registering BlockManagerMaster
[2024-02-26T15:55:13.183+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-02-26T15:55:13.184+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-02-26T15:55:13.205+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-02-26T15:55:13.224+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f4434ff4-9552-4f90-8ceb-abf2cae60bd8
[2024-02-26T15:55:13.238+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-02-26T15:55:13.256+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-02-26T15:55:13.407+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-02-26T15:55:13.457+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-02-26T15:55:13.615+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO Executor: Starting executor ID driver on host 1db92e94aa46
[2024-02-26T15:55:13.621+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-02-26T15:55:13.639+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38489.
[2024-02-26T15:55:13.640+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO NettyBlockTransferService: Server created on 1db92e94aa46:38489
[2024-02-26T15:55:13.644+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-02-26T15:55:13.652+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T15:55:13.658+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO BlockManagerMasterEndpoint: Registering block manager 1db92e94aa46:38489 with 434.4 MiB RAM, BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T15:55:13.669+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T15:55:13.670+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T15:55:14.015+0000] {spark_submit.py:490} INFO - /home/airflow/.local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.
[2024-02-26T15:55:14.015+0000] {spark_submit.py:490} INFO - warnings.warn("Python 3.7 support is deprecated in Spark 3.4.", FutureWarning)
[2024-02-26T15:55:16.415+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-02-26T15:55:16.429+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:16 INFO SharedState: Warehouse path is 'file:/opt/airflow/spark-warehouse'.
[2024-02-26T15:55:23.915+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:23 INFO CodeGenerator: Code generated in 209.342698 ms
[2024-02-26T15:55:24.124+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:24 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2024-02-26T15:55:24.138+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:24 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-02-26T15:55:24.139+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:24 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2024-02-26T15:55:24.139+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:24 INFO DAGScheduler: Parents of final stage: List()
[2024-02-26T15:55:24.140+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:24 INFO DAGScheduler: Missing parents: List()
[2024-02-26T15:55:24.149+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-02-26T15:55:24.269+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 19.9 KiB, free 434.4 MiB)
[2024-02-26T15:55:24.728+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.6 KiB, free 434.4 MiB)
[2024-02-26T15:55:24.758+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1db92e94aa46:38489 (size: 9.6 KiB, free: 434.4 MiB)
[2024-02-26T15:55:24.783+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:24 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
[2024-02-26T15:55:24.868+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:24 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-02-26T15:55:24.875+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0
[2024-02-26T15:55:25.396+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (1db92e94aa46, executor driver, partition 0, PROCESS_LOCAL, 7343 bytes)
[2024-02-26T15:55:25.460+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:25 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (1db92e94aa46, executor driver, partition 1, PROCESS_LOCAL, 112942 bytes)
[2024-02-26T15:55:25.652+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:25 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
[2024-02-26T15:55:25.664+0000] {spark_submit.py:490} INFO - 24/02/26 15:55:25 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-02-26T15:57:38.471+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:38 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@643fe170)) by listener HeartbeatReceiver took 1.177899239s.
[2024-02-26T15:57:38.479+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:38 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@1432581b)) by listener AppStatusListener took 17.794454855s.
[2024-02-26T15:57:38.508+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:38 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 132370 ms exceeds timeout 120000 ms
[2024-02-26T15:57:38.877+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:38 WARN SparkContext: Killing executors is not supported by current scheduler.
[2024-02-26T15:57:38.918+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:38 INFO Executor: Told to re-register on heartbeat
[2024-02-26T15:57:38.920+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:38 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T15:57:38.925+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T15:57:38.989+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:38 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T15:57:38.990+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:38.990+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:38.990+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:38.990+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T15:57:38.990+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T15:57:38.990+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T15:57:38.991+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T15:57:38.991+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T15:57:38.991+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T15:57:38.991+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:38.991+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T15:57:38.991+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T15:57:38.991+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T15:57:38.992+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T15:57:38.992+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T15:57:38.992+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T15:57:38.992+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T15:57:38.992+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T15:57:38.992+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:38.992+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:38.992+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:38.993+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T15:57:38.993+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T15:57:38.993+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T15:57:38.993+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T15:57:38.993+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T15:57:38.993+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T15:57:38.993+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T15:57:38.993+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T15:57:38.993+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T15:57:38.994+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T15:57:38.994+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T15:57:38.994+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T15:57:38.994+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T15:57:38.994+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T15:57:38.994+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T15:57:38.994+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T15:57:38.994+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T15:57:38.995+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T15:57:38.995+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T15:57:38.995+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T15:57:38.995+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:38.995+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:38.995+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:38.995+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:38.995+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-02-26T15:57:38.998+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-02-26T15:57:38.998+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-02-26T15:57:38.998+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-02-26T15:57:38.999+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-02-26T15:57:38.999+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-02-26T15:57:38.999+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-02-26T15:57:38.999+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-02-26T15:57:39.000+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-02-26T15:57:39.000+0000] {spark_submit.py:490} INFO - ... 17 more
[2024-02-26T15:57:39.000+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:38 ERROR Inbox: Ignoring error
[2024-02-26T15:57:39.000+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.000+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.000+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.000+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T15:57:39.000+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T15:57:39.001+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T15:57:39.012+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T15:57:39.017+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T15:57:39.017+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T15:57:39.017+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T15:57:39.018+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T15:57:39.018+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T15:57:39.018+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T15:57:39.019+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T15:57:39.019+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T15:57:39.019+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T15:57:39.020+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T15:57:39.021+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T15:57:39.021+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T15:57:39.021+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T15:57:39.021+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T15:57:39.022+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T15:57:39.022+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T15:57:39.023+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T15:57:39.027+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T15:57:39.029+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.029+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.033+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.034+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.034+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-02-26T15:57:39.035+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-02-26T15:57:39.035+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-02-26T15:57:39.035+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-02-26T15:57:39.035+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-02-26T15:57:39.036+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-02-26T15:57:39.037+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-02-26T15:57:39.037+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-02-26T15:57:39.038+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-02-26T15:57:39.038+0000] {spark_submit.py:490} INFO - ... 17 more
[2024-02-26T15:57:39.038+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO Executor: Told to re-register on heartbeat
[2024-02-26T15:57:39.039+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T15:57:39.039+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T15:57:39.039+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 ERROR Inbox: Ignoring error
[2024-02-26T15:57:39.040+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.040+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.041+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.063+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T15:57:39.064+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T15:57:39.064+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T15:57:39.064+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T15:57:39.064+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T15:57:39.065+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T15:57:39.065+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T15:57:39.065+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T15:57:39.065+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T15:57:39.065+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T15:57:39.065+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T15:57:39.066+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T15:57:39.066+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T15:57:39.066+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T15:57:39.066+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T15:57:39.066+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T15:57:39.066+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T15:57:39.066+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T15:57:39.066+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T15:57:39.066+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T15:57:39.066+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T15:57:39.067+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T15:57:39.067+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.067+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.067+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.067+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.067+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.067+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.067+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.067+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.067+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.067+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.067+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.068+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.068+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.068+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.068+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.069+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.069+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.070+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.070+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.070+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.071+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.071+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.071+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.071+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T15:57:39.072+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T15:57:39.072+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:39.072+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T15:57:39.072+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T15:57:39.072+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T15:57:39.073+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T15:57:39.073+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T15:57:39.073+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T15:57:39.073+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.073+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.074+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.074+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.074+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T15:57:39.082+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T15:57:39.083+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T15:57:39.083+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T15:57:39.084+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T15:57:39.084+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T15:57:39.085+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.085+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.085+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.085+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.086+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.086+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.086+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.086+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.087+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.087+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.087+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T15:57:39.087+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T15:57:39.088+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T15:57:39.088+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T15:57:39.088+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T15:57:39.089+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T15:57:39.089+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T15:57:39.090+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T15:57:39.094+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.095+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.095+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.095+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T15:57:39.095+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T15:57:39.095+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T15:57:39.095+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T15:57:39.096+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T15:57:39.096+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T15:57:39.096+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:39.096+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T15:57:39.096+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T15:57:39.097+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T15:57:39.097+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T15:57:39.097+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T15:57:39.098+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T15:57:39.100+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T15:57:39.100+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T15:57:39.101+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.102+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.102+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.103+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T15:57:39.103+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T15:57:39.103+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T15:57:39.103+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T15:57:39.103+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T15:57:39.108+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T15:57:39.109+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T15:57:39.109+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T15:57:39.109+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T15:57:39.109+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T15:57:39.109+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T15:57:39.110+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T15:57:39.111+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T15:57:39.112+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T15:57:39.112+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T15:57:39.112+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T15:57:39.112+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T15:57:39.116+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T15:57:39.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T15:57:39.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T15:57:39.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.120+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.121+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.121+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.121+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.121+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.121+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.121+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.121+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.122+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.122+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.122+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.125+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.125+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.125+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.125+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.126+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.126+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.129+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.130+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T15:57:39.130+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T15:57:39.130+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:39.131+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T15:57:39.132+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T15:57:39.132+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T15:57:39.132+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T15:57:39.132+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T15:57:39.132+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T15:57:39.133+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.133+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.133+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.133+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.133+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T15:57:39.134+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T15:57:39.134+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T15:57:39.134+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T15:57:39.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T15:57:39.143+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T15:57:39.143+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.143+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.143+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.144+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.144+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.144+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.144+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.144+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.144+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.144+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.145+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T15:57:39.145+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T15:57:39.146+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T15:57:39.146+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T15:57:39.146+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T15:57:39.146+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T15:57:39.152+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T15:57:39.153+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO Executor: Told to re-register on heartbeat
[2024-02-26T15:57:39.153+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T15:57:39.153+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T15:57:39.153+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T15:57:39.154+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.155+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.155+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.155+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T15:57:39.157+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T15:57:39.157+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T15:57:39.157+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T15:57:39.157+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T15:57:39.158+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T15:57:39.158+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:39.158+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T15:57:39.158+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T15:57:39.158+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T15:57:39.158+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T15:57:39.158+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T15:57:39.159+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T15:57:39.159+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T15:57:39.159+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T15:57:39.159+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.159+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.160+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.160+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T15:57:39.160+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T15:57:39.160+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T15:57:39.161+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T15:57:39.161+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T15:57:39.161+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T15:57:39.162+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T15:57:39.162+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T15:57:39.162+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T15:57:39.162+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T15:57:39.162+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T15:57:39.162+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T15:57:39.162+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T15:57:39.163+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T15:57:39.163+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T15:57:39.163+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T15:57:39.164+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T15:57:39.164+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T15:57:39.164+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T15:57:39.164+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T15:57:39.165+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.165+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.165+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.166+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.166+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.166+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.166+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.167+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.167+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.167+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.168+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.168+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.168+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.168+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.168+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.169+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.169+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.169+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.169+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.170+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.170+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.170+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.171+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.171+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T15:57:39.171+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T15:57:39.171+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:39.171+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T15:57:39.172+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T15:57:39.172+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T15:57:39.172+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T15:57:39.172+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T15:57:39.173+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T15:57:39.173+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.173+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.173+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.173+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.174+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T15:57:39.174+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T15:57:39.175+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T15:57:39.175+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T15:57:39.175+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T15:57:39.176+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T15:57:39.176+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.176+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.176+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.176+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.176+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.179+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.191+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.193+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.194+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.194+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.194+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T15:57:39.195+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T15:57:39.203+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T15:57:39.204+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T15:57:39.204+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T15:57:39.204+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T15:57:39.204+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T15:57:39.204+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 ERROR Inbox: Ignoring error
[2024-02-26T15:57:39.204+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.204+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.204+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.204+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T15:57:39.205+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T15:57:39.205+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T15:57:39.205+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T15:57:39.205+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T15:57:39.205+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T15:57:39.205+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T15:57:39.205+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T15:57:39.205+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T15:57:39.205+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T15:57:39.206+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T15:57:39.206+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T15:57:39.206+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T15:57:39.206+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T15:57:39.206+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T15:57:39.206+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T15:57:39.206+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T15:57:39.206+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T15:57:39.206+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T15:57:39.206+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T15:57:39.206+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T15:57:39.206+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T15:57:39.207+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.207+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.207+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.207+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.207+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.210+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.210+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.211+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.211+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.211+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.211+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.211+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.211+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.211+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.211+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.211+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.212+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.212+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.212+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.212+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.212+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.212+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.212+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.212+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T15:57:39.213+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T15:57:39.213+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:39.213+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T15:57:39.213+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T15:57:39.213+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T15:57:39.213+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T15:57:39.213+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T15:57:39.214+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T15:57:39.214+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.214+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.214+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.214+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.214+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T15:57:39.214+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T15:57:39.218+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T15:57:39.219+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T15:57:39.219+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T15:57:39.219+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T15:57:39.220+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.220+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.220+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.220+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.220+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.220+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.221+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.221+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.221+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.221+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.221+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T15:57:39.221+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T15:57:39.221+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T15:57:39.221+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T15:57:39.222+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T15:57:39.222+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T15:57:39.222+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T15:57:39.223+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO Executor: Told to re-register on heartbeat
[2024-02-26T15:57:39.223+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T15:57:39.223+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T15:57:39.231+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 ERROR Inbox: Ignoring error
[2024-02-26T15:57:39.231+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.231+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.231+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.231+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T15:57:39.232+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T15:57:39.232+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T15:57:39.232+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T15:57:39.249+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T15:57:39.250+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T15:57:39.250+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T15:57:39.255+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T15:57:39.256+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T15:57:39.256+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T15:57:39.257+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T15:57:39.257+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T15:57:39.258+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T15:57:39.258+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T15:57:39.258+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T15:57:39.259+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T15:57:39.259+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T15:57:39.259+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T15:57:39.259+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T15:57:39.260+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T15:57:39.260+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T15:57:39.260+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T15:57:39.261+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.261+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.262+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.262+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.262+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.262+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.263+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.263+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.263+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.264+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.265+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.265+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.266+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.266+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.266+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.268+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.270+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.270+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.271+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.271+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T15:57:39.271+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T15:57:39.274+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:39.275+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T15:57:39.279+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T15:57:39.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T15:57:39.288+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T15:57:39.288+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T15:57:39.289+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T15:57:39.289+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.289+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.289+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.289+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.289+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T15:57:39.289+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T15:57:39.289+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T15:57:39.290+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T15:57:39.290+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T15:57:39.290+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T15:57:39.290+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.290+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.291+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T15:57:39.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T15:57:39.293+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T15:57:39.293+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T15:57:39.293+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T15:57:39.293+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T15:57:39.293+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T15:57:39.293+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T15:57:39.293+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.293+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.293+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.294+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T15:57:39.294+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T15:57:39.294+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T15:57:39.294+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T15:57:39.294+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T15:57:39.296+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T15:57:39.296+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:39.297+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T15:57:39.297+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T15:57:39.297+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T15:57:39.297+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T15:57:39.297+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T15:57:39.298+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T15:57:39.298+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T15:57:39.298+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T15:57:39.298+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.298+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.298+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.299+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T15:57:39.300+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T15:57:39.300+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T15:57:39.301+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T15:57:39.302+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T15:57:39.303+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T15:57:39.303+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T15:57:39.304+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T15:57:39.304+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T15:57:39.304+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T15:57:39.304+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T15:57:39.304+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T15:57:39.304+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T15:57:39.304+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T15:57:39.304+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T15:57:39.316+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T15:57:39.321+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T15:57:39.321+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T15:57:39.321+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T15:57:39.322+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T15:57:39.322+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.323+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.324+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.324+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.324+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.324+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.326+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.326+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.326+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.326+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.328+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.328+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.331+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.331+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.331+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.331+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.331+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.331+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.331+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.332+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.332+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.332+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.332+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.332+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T15:57:39.336+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T15:57:39.336+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:39.336+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T15:57:39.336+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T15:57:39.336+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.338+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.338+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.338+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T15:57:39.338+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T15:57:39.338+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T15:57:39.338+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T15:57:39.338+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T15:57:39.338+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T15:57:39.339+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T15:57:39.339+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO Executor: Told to re-register on heartbeat
[2024-02-26T15:57:39.339+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T15:57:39.339+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T15:57:39.339+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 ERROR Inbox: Ignoring error
[2024-02-26T15:57:39.339+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.339+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.339+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.339+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T15:57:39.340+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T15:57:39.340+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T15:57:39.340+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T15:57:39.340+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T15:57:39.340+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T15:57:39.340+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T15:57:39.340+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T15:57:39.340+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T15:57:39.341+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T15:57:39.341+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T15:57:39.341+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T15:57:39.342+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T15:57:39.352+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T15:57:39.353+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T15:57:39.354+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T15:57:39.356+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T15:57:39.360+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T15:57:39.364+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T15:57:39.364+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T15:57:39.364+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T15:57:39.364+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T15:57:39.364+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.364+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.366+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.366+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.366+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.366+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.366+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.366+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.366+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:39.367+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T15:57:39.368+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.369+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.369+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.370+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.370+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.370+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.370+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.370+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.370+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.370+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.371+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T15:57:39.371+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T15:57:39.372+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T15:57:39.372+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T15:57:39.372+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T15:57:39.372+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T15:57:39.372+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T15:57:39.374+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T15:57:39.374+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.374+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.374+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.376+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.377+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.379+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T15:57:39.379+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T15:57:39.379+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T15:57:39.379+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T15:57:39.379+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T15:57:39.380+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T15:57:39.380+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T15:57:39.380+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T15:57:39.380+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T15:57:39.380+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T15:57:39.380+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T15:57:39.380+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T15:57:39.380+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T15:57:39.380+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T15:57:39.380+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T15:57:39.380+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T15:57:39.380+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T15:57:39.380+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T15:57:39.380+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T15:57:39.380+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T15:57:39.381+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.381+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.381+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.384+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.384+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.385+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.386+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.386+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.386+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.386+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.386+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.386+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.386+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.386+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.386+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.386+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.386+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.386+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.386+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.387+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.387+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.388+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.388+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.391+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T15:57:39.391+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T15:57:39.391+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:39.391+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T15:57:39.391+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T15:57:39.391+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T15:57:39.391+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T15:57:39.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T15:57:39.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T15:57:39.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T15:57:39.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T15:57:39.395+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T15:57:39.395+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T15:57:39.395+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T15:57:39.395+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T15:57:39.395+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.395+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.395+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.395+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.395+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.395+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.395+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.395+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.395+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.396+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.396+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T15:57:39.396+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T15:57:39.396+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T15:57:39.396+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T15:57:39.396+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T15:57:39.396+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T15:57:39.396+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T15:57:39.398+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO Executor: Told to re-register on heartbeat
[2024-02-26T15:57:39.398+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T15:57:39.398+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T15:57:39.398+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 ERROR Inbox: Ignoring error
[2024-02-26T15:57:39.398+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.398+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.398+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.399+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T15:57:39.399+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T15:57:39.399+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T15:57:39.399+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T15:57:39.399+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T15:57:39.399+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T15:57:39.399+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T15:57:39.399+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T15:57:39.399+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T15:57:39.399+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T15:57:39.399+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T15:57:39.399+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T15:57:39.399+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T15:57:39.399+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T15:57:39.399+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T15:57:39.400+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T15:57:39.400+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T15:57:39.400+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T15:57:39.404+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T15:57:39.404+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T15:57:39.404+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T15:57:39.404+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T15:57:39.419+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.420+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.420+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.420+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.420+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.420+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.421+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T15:57:39.422+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.423+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.423+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.423+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.423+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T15:57:39.423+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T15:57:39.423+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T15:57:39.424+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T15:57:39.424+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T15:57:39.424+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T15:57:39.424+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.424+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.424+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.424+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.424+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.424+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.424+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.424+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.424+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.424+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.424+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T15:57:39.424+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T15:57:39.425+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T15:57:39.425+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T15:57:39.425+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T15:57:39.425+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T15:57:39.425+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T15:57:39.426+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T15:57:39.426+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.426+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.426+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.432+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T15:57:39.432+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T15:57:39.434+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T15:57:39.435+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T15:57:39.437+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T15:57:39.438+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T15:57:39.439+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T15:57:39.440+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T15:57:39.440+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T15:57:39.440+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T15:57:39.441+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T15:57:39.444+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T15:57:39.444+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T15:57:39.445+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T15:57:39.445+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T15:57:39.447+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T15:57:39.456+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T15:57:39.458+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T15:57:39.475+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T15:57:39.486+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T15:57:39.486+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T15:57:39.497+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T15:57:39.498+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T15:57:39.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.546+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.546+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.547+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.548+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.548+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.549+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.550+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.550+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.551+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.551+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.559+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.565+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:39.565+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:39.565+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.579+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.592+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.592+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.593+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:39.598+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:39.612+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:39.613+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T15:57:39.614+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:39.630+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T15:57:39.637+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T15:57:39.638+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T15:57:39.638+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T15:57:39.639+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T15:57:39.640+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T15:57:39.641+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T15:57:39.641+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T15:57:39.654+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T15:57:39.683+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:39.724+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:39.780+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:39.808+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:39.813+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T15:57:39.859+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T15:57:39.892+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T15:57:40.144+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T15:57:40.289+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T15:57:40.435+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T15:57:40.668+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T15:57:40.687+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T15:57:40.713+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T15:57:40.729+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T15:57:40.749+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T15:57:40.751+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T15:57:40.753+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T15:57:40.753+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T15:57:40.754+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T15:57:40.754+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T15:57:40.754+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T15:57:40.755+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T15:57:40.756+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T15:57:40.757+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T15:57:40.759+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T15:57:40.761+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T15:57:40.763+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T15:57:40.771+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO Executor: Told to re-register on heartbeat
[2024-02-26T15:57:42.469+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T15:57:43.179+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T15:57:43.234+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 ERROR Inbox: Ignoring error
[2024-02-26T15:57:44.217+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T15:57:46.196+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T15:57:46.557+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T15:57:47.284+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T15:57:59.983+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T15:58:08.768+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T15:58:09.475+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T15:58:18.640+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T15:58:33.061+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T15:58:43.190+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T15:58:47.561+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T15:58:48.568+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T15:58:52.148+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T15:58:56.008+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T15:58:57.441+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T15:59:05.369+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T15:59:25.291+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T15:59:41.019+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:00:04.676+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:00:22.234+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:00:48.333+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:01.801+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:01.956+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:01.959+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:01.961+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:01.962+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:01.962+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:01.962+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:01.963+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:01.963+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:01.963+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:01.964+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:01.966+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:01.968+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:01.969+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:01.969+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:01.995+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.032+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.032+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.032+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.042+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.042+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.062+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.066+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.066+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.069+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.071+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:02.072+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.115+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:02.115+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:02.116+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:02.116+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:02.127+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:02.141+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:02.145+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:02.151+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:02.151+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:02.154+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.154+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.154+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.154+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.154+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:02.154+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:02.155+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:02.155+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:02.155+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:02.156+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:02.156+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.156+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.156+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.157+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.158+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.160+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.163+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.163+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.163+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.163+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.164+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:02.164+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:02.164+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:02.164+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:02.164+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:02.165+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:02.251+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:02.251+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:02.251+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:02.256+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:02.256+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:02.265+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:02.271+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:02.271+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:02.271+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:02.271+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:02.271+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:02.272+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:02.272+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:02.272+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:02.272+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:02.272+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:02.272+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:02.273+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:02.273+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:02.273+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:02.273+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:02.273+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:02.275+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:02.275+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:02.279+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:02.279+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:02.279+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:02.279+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:02.279+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:02.280+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:02.280+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:02.281+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:02.282+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:02.283+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:02.283+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:02.284+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:02.285+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:02.285+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:02.323+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:02.323+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:02.323+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:02.323+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:02.323+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:02.323+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.324+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.329+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.329+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.332+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.332+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.332+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.332+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.332+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.337+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.339+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:02.339+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.340+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.385+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.387+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.387+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.387+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.388+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.388+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.388+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.388+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.388+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:02.388+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.388+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:02.389+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:02.389+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:02.389+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:02.389+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:02.389+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:02.390+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:02.390+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:02.390+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:02.391+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.391+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.391+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.391+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.392+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:02.392+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:02.392+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:02.392+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:02.393+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:02.393+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:02.393+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.393+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.393+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.393+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:02.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:02.394+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:02.395+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:02.395+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:02.395+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:02.396+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:02.396+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:02.400+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:02.400+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:02.400+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 ERROR Inbox: Ignoring error
[2024-02-26T16:01:02.400+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:02.401+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:02.401+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:02.401+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:02.401+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:02.401+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:02.404+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:02.404+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:02.404+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:02.404+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:02.404+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:02.405+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:02.405+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:02.405+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:02.405+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:02.405+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:02.405+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:02.406+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:02.406+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:02.406+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:02.406+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:02.406+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:02.406+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:02.407+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:02.407+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:02.407+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.407+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.407+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.407+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.407+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:02.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.409+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.409+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.409+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.410+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.410+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.410+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.410+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.410+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.410+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.410+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:02.410+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.411+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:02.416+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:02.419+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:02.436+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:02.441+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:02.449+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:02.472+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:02.489+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:02.489+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:02.489+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.489+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.490+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.491+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.491+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:02.491+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:02.491+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:02.491+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:02.492+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:02.492+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:02.492+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.493+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.493+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.494+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.494+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.494+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.495+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.495+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.496+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.496+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.497+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:02.497+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:02.497+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:02.497+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:02.498+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:02.498+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:02.498+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:02.499+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:02.499+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:02.499+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:02.500+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:02.500+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:02.500+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:02.500+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:02.501+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:02.501+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:02.501+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:02.501+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:02.502+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:02.505+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:02.505+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:02.505+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:02.505+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:02.505+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:02.505+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:02.505+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:02.506+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:02.506+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:02.507+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:02.507+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:02.507+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:02.508+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:02.508+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:02.508+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:02.508+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:02.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:02.520+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:02.521+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:02.521+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:02.521+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:02.521+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:02.521+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:02.521+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:02.522+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:02.523+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:02.523+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:02.523+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:02.523+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:02.524+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:02.524+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.524+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.524+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.525+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.525+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.525+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.525+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.525+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.525+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.528+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.528+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:02.528+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.528+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.528+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.528+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.529+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.529+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.529+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.529+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.529+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.529+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.530+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:02.530+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.530+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:02.530+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:02.530+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:02.530+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:02.531+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:02.531+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:02.531+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:02.531+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:02.531+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:02.531+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.531+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.532+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.532+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.532+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:02.532+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:02.532+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:02.533+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:02.533+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:02.533+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:02.533+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.533+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.533+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.534+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.534+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.534+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.534+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.534+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.534+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.534+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.535+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:02.535+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:02.535+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:02.535+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:02.535+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:02.535+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:02.536+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:02.536+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:02.536+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:02.536+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:02.536+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 ERROR Inbox: Ignoring error
[2024-02-26T16:01:02.537+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:02.537+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:02.537+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:02.537+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:02.537+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:02.537+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:02.538+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:02.538+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:02.538+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:02.539+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:02.539+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:02.539+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:02.540+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:02.540+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:02.540+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:02.540+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:02.540+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:02.540+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:02.541+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:02.541+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:02.541+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:02.541+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:02.541+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:02.542+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:02.542+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:02.542+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.542+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.542+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.542+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.546+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.546+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.546+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.546+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.546+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.547+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.547+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:02.547+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.547+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.547+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.547+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.547+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.548+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.548+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.548+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.548+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.548+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.548+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:02.549+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.549+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:02.549+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:02.549+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:02.549+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:02.551+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:02.552+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:02.552+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:02.552+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:02.556+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:02.556+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.557+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.557+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.557+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.557+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:02.557+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:02.557+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:02.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:02.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:02.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:02.558+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.558+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.558+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.559+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.559+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.559+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.560+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.560+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.560+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.560+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:02.561+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:02.561+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:02.562+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:02.562+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:02.564+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:02.565+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:02.566+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:02.566+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:02.566+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:02.566+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:02.567+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:02.567+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:02.567+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:02.568+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:02.569+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:02.570+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:02.570+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:02.570+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:02.571+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:02.571+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:02.571+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:02.571+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:02.571+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:02.572+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:02.572+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:02.572+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:02.572+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:02.572+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:02.573+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:02.573+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:02.574+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:02.583+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:02.583+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:02.585+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:02.587+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:02.587+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:02.589+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:02.589+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:02.592+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:02.592+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:02.593+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:02.593+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:02.595+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:02.599+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:02.650+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:02.707+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:02.725+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:02.731+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:02.732+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.734+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.736+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.736+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.737+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.737+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.762+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.764+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.765+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.765+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.765+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:02.765+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.766+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.767+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.767+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.767+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.767+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.768+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.768+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.768+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.770+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.771+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:02.771+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.771+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:02.772+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:02.772+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:02.772+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:02.772+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:02.772+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:02.772+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:02.773+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:02.774+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:02.774+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.774+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.774+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.774+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.775+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:02.775+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:02.775+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:02.775+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:02.775+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:02.775+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:02.777+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.777+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.777+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.777+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.777+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.777+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.778+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.779+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.779+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.779+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.779+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:02.780+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:02.781+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:02.781+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:02.784+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:02.787+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:02.789+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:02.790+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:02.790+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:02.792+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:02.792+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 ERROR Inbox: Ignoring error
[2024-02-26T16:01:02.793+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:02.797+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:02.798+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:02.799+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:02.801+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:02.801+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:02.801+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:02.803+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:02.804+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:02.805+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:02.806+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:02.807+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:02.808+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:02.833+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:02.844+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:02.847+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:02.848+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:02.848+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:02.848+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:02.849+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:02.849+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:02.851+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:02.864+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:02.872+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:02.873+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:02.874+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.875+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.876+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.877+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.877+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.878+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.879+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.880+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.882+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.895+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.896+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:02.896+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.896+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.897+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.897+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.897+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.897+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.899+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.899+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.900+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.900+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.901+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:02.902+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.903+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:02.904+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:02.905+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:02.905+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:02.905+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:02.905+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:02.907+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:02.907+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:02.908+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:02.908+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.908+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.909+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.910+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.911+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:02.911+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:02.911+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:02.911+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:02.911+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:02.912+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:02.912+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.914+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.914+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.917+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.918+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.918+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.918+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.918+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.918+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.919+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.919+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:02.919+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:02.920+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:02.952+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:02.954+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:02.954+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:02.954+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:02.955+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:02.956+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:02.956+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:02.956+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:02.956+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:02.956+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:02.957+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:02.958+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:02.958+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:02.958+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:02.959+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:02.960+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:02.960+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:02.960+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:02.960+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:02.961+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:02.962+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:02.962+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:02.962+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:02.962+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:02.963+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:02.963+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:02.964+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:02.971+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:02.971+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:02.971+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:02.972+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:02.972+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:02.972+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:02.973+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:02.973+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:02.973+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:02.973+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:02.974+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:02.974+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:02.975+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:02.975+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:02.979+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:02.979+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:02.979+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:02.980+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:02.981+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:02.988+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.988+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.989+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.989+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.989+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.989+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.990+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.990+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.990+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.990+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.990+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:02.990+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.990+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.990+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.990+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.990+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.991+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.991+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.991+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.991+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.991+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.991+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:02.991+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.991+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:02.991+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:02.991+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:02.991+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:02.992+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:02.992+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:02.992+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:02.993+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:02.993+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:02.993+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.993+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.993+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.994+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.994+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:02.994+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:02.994+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:02.994+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:02.994+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:02.995+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:02.995+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:02.995+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:02.995+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:02.996+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:02.996+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:02.996+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:02.996+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:02.997+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:02.997+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:02.998+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:02.998+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:02.998+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:02.999+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:03.000+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:03.000+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:03.000+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:03.000+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:03.001+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:03.001+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:03.001+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:03.002+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 ERROR Inbox: Ignoring error
[2024-02-26T16:01:03.002+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.002+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.003+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.003+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.003+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.003+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.006+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.008+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.009+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.009+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.009+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.010+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.010+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.010+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.010+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.015+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.015+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.015+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.015+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.015+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.016+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.016+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.017+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.017+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.017+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.017+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.017+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.017+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.017+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.017+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.018+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.018+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.018+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.018+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.028+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.029+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.030+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.030+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.031+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.031+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.031+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.031+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.031+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.032+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.032+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.033+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.033+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.034+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.035+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:03.035+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:03.035+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.035+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:03.036+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:03.037+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:03.038+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:03.038+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:03.038+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:03.041+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.041+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.042+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.042+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.055+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:03.056+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:03.057+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:03.081+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:03.081+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:03.082+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:03.082+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.082+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.093+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.093+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.093+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.093+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.093+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.093+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.095+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.095+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.095+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:03.095+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:03.095+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:03.095+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:03.095+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:03.096+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:03.096+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:03.096+0000] {spark_submit.py:490} INFO - 24/02/26 16:00:37 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@5e419bfc)) by listener HeartbeatReceiver took 2.560177564s.
[2024-02-26T16:01:03.096+0000] {spark_submit.py:490} INFO - 24/02/26 16:00:27 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@3ba47fe0)) by listener AppStatusListener took 51.224199349s.
[2024-02-26T16:01:03.096+0000] {spark_submit.py:490} INFO - 24/02/26 15:57:39 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:03.096+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.096+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.097+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.097+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:03.097+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:03.097+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:03.097+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:03.097+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:03.097+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:03.097+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.098+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:03.098+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:03.098+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:03.098+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:03.098+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:03.098+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.098+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.098+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.098+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.098+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.098+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.098+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.099+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.099+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.099+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.099+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.099+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.099+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.099+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.099+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.099+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.099+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.099+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.099+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.099+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.100+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:03.100+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.100+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.100+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.100+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.100+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.100+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.100+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.100+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.100+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.100+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.101+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.101+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.101+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.101+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.101+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.101+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.102+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.102+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.102+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.102+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.102+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.102+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.103+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.103+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.103+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.103+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.103+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.104+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.104+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:03.104+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:03.104+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.104+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:03.104+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:03.105+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:03.105+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:03.105+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:03.105+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:03.105+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.105+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.106+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.106+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.106+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:03.106+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:03.106+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:03.107+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:03.107+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:03.107+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:03.107+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.107+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.107+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.107+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.108+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.108+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.108+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.108+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.108+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.108+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.108+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:03.108+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:03.108+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:03.108+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:03.108+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:03.109+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:03.110+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:03.110+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:02 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:03.110+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:02 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:03.110+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:03.110+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 ERROR Inbox: Ignoring error
[2024-02-26T16:01:03.110+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.110+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.111+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.111+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.111+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.111+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.111+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.111+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.111+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.111+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.111+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.112+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.112+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.112+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.112+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.112+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.112+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.112+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.113+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.113+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.113+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.113+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.113+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.113+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.113+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.113+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.114+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.114+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.114+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.114+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-02-26T16:01:03.114+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-02-26T16:01:03.115+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-02-26T16:01:03.115+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-02-26T16:01:03.115+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-02-26T16:01:03.115+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-02-26T16:01:03.116+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-02-26T16:01:03.119+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-02-26T16:01:03.126+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-02-26T16:01:03.138+0000] {spark_submit.py:490} INFO - ... 17 more
[2024-02-26T16:01:03.138+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:03.138+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.138+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.139+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.139+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:03.145+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:03.145+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:03.145+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:03.145+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:03.145+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:03.145+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.146+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:03.146+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:03.146+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:03.146+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:03.146+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:03.146+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.146+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.146+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.147+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.147+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.147+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.147+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.147+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.147+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.147+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.148+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.148+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.148+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.148+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.148+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.148+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.148+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.149+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.149+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.149+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.149+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:03.149+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.149+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.149+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.149+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.150+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.150+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.150+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.150+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.150+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.150+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-02-26T16:01:03.150+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-02-26T16:01:03.150+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-02-26T16:01:03.151+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-02-26T16:01:03.151+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-02-26T16:01:03.151+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-02-26T16:01:03.156+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-02-26T16:01:03.157+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-02-26T16:01:03.157+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-02-26T16:01:03.157+0000] {spark_submit.py:490} INFO - ... 17 more
[2024-02-26T16:01:03.158+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:03.158+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:03.158+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:03.158+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 ERROR Inbox: Ignoring error
[2024-02-26T16:01:03.158+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.158+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.159+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.159+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.159+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.159+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.159+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.160+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.160+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.166+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.167+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.167+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.167+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.167+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.167+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.181+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.203+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.203+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.203+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.203+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.217+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.217+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.217+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.218+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.218+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.218+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.218+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.218+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.218+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.218+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-02-26T16:01:03.218+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-02-26T16:01:03.218+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-02-26T16:01:03.219+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-02-26T16:01:03.219+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-02-26T16:01:03.219+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-02-26T16:01:03.219+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-02-26T16:01:03.219+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-02-26T16:01:03.219+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-02-26T16:01:03.219+0000] {spark_submit.py:490} INFO - ... 17 more
[2024-02-26T16:01:03.220+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:03.220+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.220+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.220+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.220+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:03.220+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:03.220+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:03.220+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:03.220+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:03.220+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:03.220+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.221+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:03.221+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:03.221+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:03.221+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:03.221+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:03.221+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.221+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.221+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.221+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.221+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.221+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.222+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.222+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.222+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.222+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.222+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.222+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.224+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.224+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.224+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.224+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.224+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.224+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.224+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.225+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.225+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:03.225+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.225+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.225+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.226+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.226+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.226+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.226+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.226+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.227+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.227+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-02-26T16:01:03.227+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-02-26T16:01:03.227+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-02-26T16:01:03.227+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-02-26T16:01:03.227+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-02-26T16:01:03.228+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-02-26T16:01:03.228+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-02-26T16:01:03.228+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-02-26T16:01:03.229+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-02-26T16:01:03.229+0000] {spark_submit.py:490} INFO - ... 17 more
[2024-02-26T16:01:03.229+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:03.230+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:03.230+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:03.230+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 ERROR Inbox: Ignoring error
[2024-02-26T16:01:03.230+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.230+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.230+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.230+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.230+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.231+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.231+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.231+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.231+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.232+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.232+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.232+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.232+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.233+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.233+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.233+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.233+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.233+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.234+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.249+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.250+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.250+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.250+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.250+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.250+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.250+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.250+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.250+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.250+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.250+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-02-26T16:01:03.250+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-02-26T16:01:03.251+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-02-26T16:01:03.251+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-02-26T16:01:03.251+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-02-26T16:01:03.252+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-02-26T16:01:03.253+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-02-26T16:01:03.253+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-02-26T16:01:03.253+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-02-26T16:01:03.253+0000] {spark_submit.py:490} INFO - ... 17 more
[2024-02-26T16:01:03.253+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:03.253+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.254+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.254+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.255+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:03.255+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:03.255+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:03.255+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:03.256+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:03.256+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:03.260+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.261+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:03.262+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:03.262+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:03.262+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:03.262+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:03.262+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.262+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.262+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.262+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.262+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.262+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.262+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.263+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.263+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.263+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.263+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.263+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.263+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.263+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.263+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.263+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.263+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.264+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.264+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.264+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.264+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:03.264+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.264+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.264+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.264+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.265+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.265+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-02-26T16:01:03.265+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-02-26T16:01:03.265+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-02-26T16:01:03.265+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-02-26T16:01:03.265+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-02-26T16:01:03.265+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-02-26T16:01:03.265+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-02-26T16:01:03.265+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-02-26T16:01:03.265+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-02-26T16:01:03.266+0000] {spark_submit.py:490} INFO - ... 17 more
[2024-02-26T16:01:03.266+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:03.266+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:03.266+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:03.266+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 ERROR Inbox: Ignoring error
[2024-02-26T16:01:03.266+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.266+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.266+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.267+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.267+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.267+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.267+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.267+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.267+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.267+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.267+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.267+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.267+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.269+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.269+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.270+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.270+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.270+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.270+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.270+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.270+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.270+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.270+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.271+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.271+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.271+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.272+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.272+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.272+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.272+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.272+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.272+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.272+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.272+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.273+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.273+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.273+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.273+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.273+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.273+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.273+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.273+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.274+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.274+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.274+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.274+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.274+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.275+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.275+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:03.275+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:03.276+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.278+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:03.278+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:03.278+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:03.278+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:03.278+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:03.279+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:03.279+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.279+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.279+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.279+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.279+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:03.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:03.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:03.280+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:03.280+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:03.280+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:03.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.280+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.281+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.281+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.281+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.281+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.281+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.281+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.281+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.282+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:03.282+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:03.282+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:03.282+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:03.283+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:03.283+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:03.283+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:03.283+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:03.283+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.284+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.284+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.284+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:03.284+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:03.284+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:03.285+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:03.285+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:03.285+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:03.285+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.285+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:03.286+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:03.286+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:03.286+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:03.286+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:03.286+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.286+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.286+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.287+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.287+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.287+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.287+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.288+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.288+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.288+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.289+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.289+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.290+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.290+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.290+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.290+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.290+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.290+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.291+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.291+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.293+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:03.293+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.293+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.293+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.293+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.294+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.294+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.294+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.295+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.295+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.295+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.295+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.296+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.296+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.296+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.296+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.296+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.296+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.297+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.297+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.297+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.297+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.298+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.309+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.310+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.310+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.311+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.311+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.311+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.311+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:03.311+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:03.315+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.315+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:03.315+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:03.315+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:03.316+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:03.316+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:03.316+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:03.316+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.316+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.316+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.316+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.317+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:03.317+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:03.317+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:03.317+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:03.317+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:03.317+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:03.319+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.319+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.319+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.319+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.320+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.320+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.320+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.320+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.320+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.320+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.322+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:03.322+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:03.324+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:03.324+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:03.327+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:03.328+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:03.331+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:03.332+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:03.332+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:03.332+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:03.332+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 ERROR Inbox: Ignoring error
[2024-02-26T16:01:03.332+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.333+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.333+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.334+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.334+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.334+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.334+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.337+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.339+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.339+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.340+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.340+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.340+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.343+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.357+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.357+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.357+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.357+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.357+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.357+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.357+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.357+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.358+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.358+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.358+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.358+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.358+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.358+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.358+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.358+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.358+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.358+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.358+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.359+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.359+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.359+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.359+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.359+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.359+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.359+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.360+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.360+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.360+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.360+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.360+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.360+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.360+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.360+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.360+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:03.360+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:03.361+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.361+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:03.361+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:03.361+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:03.361+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:03.361+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:03.361+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:03.362+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.363+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.363+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.363+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.363+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:03.365+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:03.365+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:03.365+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:03.365+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:03.365+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:03.365+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.365+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.365+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.365+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.365+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.365+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.365+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.366+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.366+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.366+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.366+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:03.366+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:03.366+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:03.366+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:03.366+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:03.366+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:03.366+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:03.366+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:03.366+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.367+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.367+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.367+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:03.367+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:03.367+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:03.368+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:03.368+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:03.368+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:03.368+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.368+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:03.368+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:03.368+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:03.369+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:03.369+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:03.372+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.374+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.375+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.383+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.385+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.385+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.385+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.385+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.385+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.385+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.387+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.387+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.387+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.388+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.388+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.391+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.394+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.395+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.395+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.396+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.396+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:03.396+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.396+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.396+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.397+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.397+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.397+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.397+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.397+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.398+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.398+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.398+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.399+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.401+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.402+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.404+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.404+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.404+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.404+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.404+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.404+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.404+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.404+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.404+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.405+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.405+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:03.406+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:03.406+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.406+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:03.406+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:03.407+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:03.407+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:03.407+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:03.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:03.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:03.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:03.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:03.408+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:03.408+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:03.408+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:03.408+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.408+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.409+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.409+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.409+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.410+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.411+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.414+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.414+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.414+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.415+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:03.415+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:03.415+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:03.415+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:03.415+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:03.417+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:03.417+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:03.417+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:03.417+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:03.417+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:03.418+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 ERROR Inbox: Ignoring error
[2024-02-26T16:01:03.418+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.418+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.418+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.419+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.419+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.419+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.419+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.419+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.420+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.420+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.420+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.421+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.421+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.421+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.425+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.425+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.426+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.426+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.426+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.426+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.426+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.426+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.426+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.426+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.427+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.427+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.427+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.427+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.427+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.427+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.428+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.428+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.428+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.428+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.429+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.431+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.432+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.432+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.433+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.433+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.433+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.435+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.435+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.436+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.436+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.436+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.437+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.438+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.438+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:03.438+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:03.438+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.440+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:03.440+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:03.440+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:03.441+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:03.441+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:03.442+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:03.442+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.444+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.444+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.445+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.447+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:03.462+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:03.463+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:03.463+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:03.464+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:03.464+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:03.464+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.464+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.486+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.486+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.486+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.496+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.496+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.497+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.497+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.497+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.498+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:03.498+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:03.498+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:03.498+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:03.499+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:03.499+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:03.499+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:03.499+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:03.500+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.500+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.501+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.501+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:03.501+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:03.502+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:03.502+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:03.503+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:03.503+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:03.503+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.503+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:03.503+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:03.503+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:03.503+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:03.503+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:03.503+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.503+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.503+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.504+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.504+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.507+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.507+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.507+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.507+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.508+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.508+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.508+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.508+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.508+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.510+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.510+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.510+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:03.510+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.513+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.513+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.513+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.513+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.513+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.514+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.514+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.514+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.514+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.514+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.515+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.517+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.517+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.517+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.517+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.518+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.518+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.518+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.518+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.519+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.520+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.520+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.521+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.521+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:03.521+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:03.522+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.522+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:03.522+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:03.522+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:03.522+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:03.522+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:03.522+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:03.523+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.523+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.524+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.524+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.524+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:03.524+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:03.524+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:03.525+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:03.525+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:03.525+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:03.526+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.526+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.527+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.527+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.527+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.527+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.527+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.527+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.528+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.528+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.528+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:03.528+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:03.528+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:03.528+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:03.528+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:03.528+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:03.529+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:03.551+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:03.564+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:03.564+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:03.593+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 ERROR Inbox: Ignoring error
[2024-02-26T16:01:03.595+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.595+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.595+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.595+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.595+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.595+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.595+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.595+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.595+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.595+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.596+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.596+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.605+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.605+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.605+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.605+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.605+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.605+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.605+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.605+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.605+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.606+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.606+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.606+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.606+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.606+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.607+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.607+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.607+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.607+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.607+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.607+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.607+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.607+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.608+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.608+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.608+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.608+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.608+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.608+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.663+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.664+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.676+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.678+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.687+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.695+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.695+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.695+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.696+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:03.696+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:03.697+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.697+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:03.698+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:03.705+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:03.706+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:03.707+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:03.709+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:03.710+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.711+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.712+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.712+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.713+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:03.727+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:03.728+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:03.732+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:03.733+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:03.735+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:03.736+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.737+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.739+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.740+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.740+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.740+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.745+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.745+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.745+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.745+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.746+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:03.746+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:03.746+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:03.746+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:03.746+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:03.746+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:03.746+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:03.747+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:03.747+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.747+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.747+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.747+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:03.748+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:03.748+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:03.749+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:03.749+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:03.749+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:03.750+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.750+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:03.751+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:03.751+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:03.751+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:03.751+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:03.752+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.752+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.752+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.753+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.753+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.753+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.753+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.754+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.754+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.755+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.755+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.755+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.756+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.756+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.758+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.759+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.759+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.759+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.760+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.763+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.767+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:03.768+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.769+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.770+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.770+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.771+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.771+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.771+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.772+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.779+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.779+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.780+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.780+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.780+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.780+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.781+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.781+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.781+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.782+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.783+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.783+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.784+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.784+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.784+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.785+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.785+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.785+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.787+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.787+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.788+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:03.788+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:03.793+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.793+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:03.793+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:03.803+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:03.803+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:03.803+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:03.804+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:03.805+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.805+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.805+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.806+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.807+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:03.808+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:03.808+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:03.808+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:03.808+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:03.814+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:03.814+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.814+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.814+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.819+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.821+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.821+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.822+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.822+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.822+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.822+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.823+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:03.823+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:03.823+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:03.824+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:03.824+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:03.825+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:03.825+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:03.826+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:03.826+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:03.826+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:03.827+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 ERROR Inbox: Ignoring error
[2024-02-26T16:01:03.827+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.828+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.828+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.828+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.829+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.829+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.829+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.830+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.830+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.831+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.831+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.831+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.890+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.893+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.897+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.898+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.898+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.899+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.899+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.900+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.902+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.903+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.905+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.907+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.907+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.907+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.911+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.911+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.911+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.912+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.912+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.912+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.914+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.915+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.915+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.915+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.916+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.916+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.916+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.916+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.917+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.917+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.917+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.917+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.918+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.918+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.918+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.918+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.919+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:03.919+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:03.919+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.920+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:03.920+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:03.927+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:03.927+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:03.927+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:03.928+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:03.928+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.928+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.928+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.929+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.929+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:03.931+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:03.931+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:03.931+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:03.932+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:03.932+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:03.932+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.932+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.933+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.933+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.933+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.934+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.934+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.934+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.935+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.935+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.935+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:03.935+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:03.935+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:03.940+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:03.941+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:03.942+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:03.942+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:03.942+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:03.943+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.943+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.943+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.944+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:03.944+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:03.945+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:03.945+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:03.945+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:03.946+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:03.946+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.946+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:03.947+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:03.947+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:03.947+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:03.948+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:03.948+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:03.949+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:03.949+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:03.949+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:03.949+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:03.950+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:03.950+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:03.951+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:03.951+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:03.951+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:03.952+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:03.952+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:03.953+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:03.953+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:03.953+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:03.954+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:03.954+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:03.954+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:03.954+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:03.955+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:03.955+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:03.955+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:03.956+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:03.956+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:03.956+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:03.956+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:03.957+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.957+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.957+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.958+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.958+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.958+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.959+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.959+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.960+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.960+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.960+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.961+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.961+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.962+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.962+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.962+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.963+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.963+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.963+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.964+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.964+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.965+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:03.965+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.965+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:03.966+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:03.966+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:03.978+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:03.979+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:03.979+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:03.980+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:03.980+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:03.981+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:03.981+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.982+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.982+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.983+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.983+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:03.983+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:03.984+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:03.984+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:03.985+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:03.986+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:03.986+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:03.987+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:03.988+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:03.988+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:03.988+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:03.989+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:03.990+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:03.990+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:03.991+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:03.991+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:03.992+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:03.992+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:03.993+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:03.994+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:03.994+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:03.994+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:03.995+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:03.996+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:03.996+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:03.996+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:04.007+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 ERROR Inbox: Ignoring error
[2024-02-26T16:01:04.007+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.008+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.009+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.010+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.010+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.011+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.011+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.012+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.012+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.013+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.014+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.014+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.015+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.015+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.016+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.016+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.017+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.017+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.026+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.026+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.027+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.027+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.028+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.028+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.031+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.031+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.032+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.032+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.033+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.033+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.034+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.034+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.034+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.036+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.037+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.037+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.038+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.078+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.079+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.080+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.081+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.083+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.093+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.102+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.103+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.103+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.104+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.105+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.105+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.105+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.106+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.106+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.107+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.108+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.108+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.109+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.109+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.110+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.110+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.110+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.111+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.111+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.112+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.112+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.113+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.113+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.114+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.114+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.115+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.116+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.116+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.116+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.118+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.118+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.119+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.119+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.119+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.120+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.120+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.120+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.121+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.121+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.122+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.122+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:04.123+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.123+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.123+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.124+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:04.124+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:04.124+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:04.125+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:04.125+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:04.126+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:04.126+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.126+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:04.131+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:04.132+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:04.132+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:04.133+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:04.133+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.134+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.134+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.134+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.136+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.136+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.137+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.137+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.138+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.138+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.138+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.139+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.139+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.139+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.151+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.152+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.153+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:04.153+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.154+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.154+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.155+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.156+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.156+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.157+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.157+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.157+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.158+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.158+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.162+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.162+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.163+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.163+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.163+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.164+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.164+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.164+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.165+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.165+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.165+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.167+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.168+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.168+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.169+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.170+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.171+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.171+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.174+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.174+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.175+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.175+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.176+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.176+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.179+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.181+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.187+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.187+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.188+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.188+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.189+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.189+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.189+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.189+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.189+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.190+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.191+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.191+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.191+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.191+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.191+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.191+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.192+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.192+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.192+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.193+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.193+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.193+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.193+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.194+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.194+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.199+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.199+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.199+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:04.199+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:04.199+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:04.200+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 ERROR Inbox: Ignoring error
[2024-02-26T16:01:04.200+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.200+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.200+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.200+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.200+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.200+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.201+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.201+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.201+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.202+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.202+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.202+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.203+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.203+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.203+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.203+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.204+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.204+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.204+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.205+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.205+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.205+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.205+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.205+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.206+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.206+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.207+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.207+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.208+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.208+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.209+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.210+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.210+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.211+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.217+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.218+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.231+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.234+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.234+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.235+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.236+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.236+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.236+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.237+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.237+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.237+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.237+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.238+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.238+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.238+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.238+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.238+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.238+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.238+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.238+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.238+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.238+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.238+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.238+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.238+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.238+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.239+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.239+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.239+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.239+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.239+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.239+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.239+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.239+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.239+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.239+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.239+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.239+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.240+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.240+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.240+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.240+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.240+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.240+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.240+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.245+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.246+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.246+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.246+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.246+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:04.246+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.246+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.246+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.247+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:04.247+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:04.247+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:04.247+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:04.247+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:04.247+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:04.247+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.248+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:04.248+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:04.248+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:04.248+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:04.248+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:04.248+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.248+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.248+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.248+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.249+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.249+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.249+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.249+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.249+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.249+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.249+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.249+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.249+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.249+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.249+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.250+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.250+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.250+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.250+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.250+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.250+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:04.250+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.250+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.250+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.250+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.250+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.250+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.250+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.250+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.250+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.251+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.251+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.251+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.251+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.251+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.251+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.251+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.251+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.251+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.251+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.251+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.252+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.252+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.252+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.252+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.252+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.252+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.252+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.252+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.252+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.252+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.252+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.252+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.252+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.252+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.253+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.254+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.254+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.254+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.254+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.254+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.254+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.254+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.254+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.254+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.254+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.254+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.254+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.254+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.255+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.255+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:04.255+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:04.255+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:04.255+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 ERROR Inbox: Ignoring error
[2024-02-26T16:01:04.255+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.255+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.255+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.255+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.255+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.255+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.255+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.255+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.255+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.255+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.256+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.256+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.256+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.256+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.256+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.256+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.256+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.256+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.256+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.256+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.256+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.256+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.256+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.257+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.257+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.257+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.257+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.257+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.257+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.257+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.257+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.257+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.257+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.257+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.257+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.257+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.257+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.258+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.258+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.259+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.259+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.259+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.259+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.259+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.260+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.260+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.260+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.260+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.261+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.261+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.261+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.261+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.261+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.261+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.261+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.261+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.261+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.262+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.262+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.262+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.262+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.262+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.262+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.263+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.263+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.263+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.263+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.263+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.263+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.263+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.264+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.265+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.265+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.265+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.265+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.265+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.265+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.265+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:04.266+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.266+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.266+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.266+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:04.266+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:04.266+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:04.266+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:04.266+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:04.266+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:04.266+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.267+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:04.267+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:04.267+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:04.267+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:04.267+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:04.269+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.269+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.269+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.269+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.269+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.269+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.269+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.270+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.270+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.270+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.270+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.270+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.270+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.270+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.270+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.270+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.270+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.270+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.271+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.271+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.271+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:04.271+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.271+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.272+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.272+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.272+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.272+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.272+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.272+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.273+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.273+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.273+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.273+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.273+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.279+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.279+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.279+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.280+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.280+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.291+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.291+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.291+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.291+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.291+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.291+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.291+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.292+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.292+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.292+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.292+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.292+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.293+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.293+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.293+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.293+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.293+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.293+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.293+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.293+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.293+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.293+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.293+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.293+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.294+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.294+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.294+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:04.294+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:04.294+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:04.294+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 ERROR Inbox: Ignoring error
[2024-02-26T16:01:04.294+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.294+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.294+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.294+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.294+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.295+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.295+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.295+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.295+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.295+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.296+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.296+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.296+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.296+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.296+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.296+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.297+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.297+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.297+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.297+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.297+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.297+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.298+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.298+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.298+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.298+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.298+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.298+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.298+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.298+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.299+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.299+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.299+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.299+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.299+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.299+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.299+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.299+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.300+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.300+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.300+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.300+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.300+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.300+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.300+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.300+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.301+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.301+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.301+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.301+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.301+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.301+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.302+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.302+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.302+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.302+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.302+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.302+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.302+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.302+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.303+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.303+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.303+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.303+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.303+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.303+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.303+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.303+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.304+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.304+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.304+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.304+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.304+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.304+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.304+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.304+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.304+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.305+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.305+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.305+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.305+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.305+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.305+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.305+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.305+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:03 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:04.305+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.306+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.306+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.306+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:04.306+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:04.306+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:04.306+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:04.306+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:04.306+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:04.307+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.307+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:04.307+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:04.307+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:04.307+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:04.307+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:04.307+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.307+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.308+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.308+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.308+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.308+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.308+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.309+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.309+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.309+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.309+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.310+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.310+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.310+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.310+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.310+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.310+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.310+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.311+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.311+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.311+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:04.311+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.311+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.311+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.312+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.312+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.312+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.312+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.312+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.312+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.312+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.312+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.312+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.313+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.313+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.313+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.313+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.313+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.313+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.313+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.313+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.313+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.313+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.313+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.313+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.314+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.314+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.314+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.314+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.314+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.314+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.314+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.314+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.315+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.315+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.315+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.315+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.315+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.315+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.315+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.316+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.316+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.316+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.316+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.316+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.316+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.316+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.316+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.316+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.316+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.317+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.317+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.345+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.349+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.360+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.363+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.364+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.365+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.365+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.365+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.365+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.366+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.366+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.366+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.367+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.367+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:04.367+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:04.367+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:04.367+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 ERROR Inbox: Ignoring error
[2024-02-26T16:01:04.368+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.368+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.368+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.371+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.371+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.371+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.371+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.371+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.371+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.373+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.374+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.389+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.390+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.391+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.391+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.392+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.393+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.394+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.395+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.395+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.395+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.396+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.396+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.396+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.396+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.396+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.396+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.396+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.396+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.396+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.397+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.397+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.398+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.398+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.398+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.399+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.399+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.399+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.399+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.399+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.399+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.399+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.399+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.399+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.400+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.400+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.400+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.400+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.402+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.402+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.403+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.403+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.417+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.418+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.420+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.420+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.421+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:04.422+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.422+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.422+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.422+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:04.422+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:04.422+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:04.422+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:04.422+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:04.423+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:04.423+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.423+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:04.423+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:04.423+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:04.423+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:04.423+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:04.424+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.432+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.432+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.432+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.432+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.432+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.432+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.433+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.441+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.443+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.443+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.443+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.443+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.443+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.443+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.443+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.444+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.444+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.444+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.444+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.444+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:04.444+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.444+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.444+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.444+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.444+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.445+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.445+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.445+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.446+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.446+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.446+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.446+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.446+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.459+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.460+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.460+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.461+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.461+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.474+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.493+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.502+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.503+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.503+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.503+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.503+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.503+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.503+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.503+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.504+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.504+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.504+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.504+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.504+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.504+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.505+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.505+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.505+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.505+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.505+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.505+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.505+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.505+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.506+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.506+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.506+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.506+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.506+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.506+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.506+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.506+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.506+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.506+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.506+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.506+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.507+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.507+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.507+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.507+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.507+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.507+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.507+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.507+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.508+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.508+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.508+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO PythonRunner: Times: total = 337899, boot = 337734, init = 162, finish = 3
[2024-02-26T16:01:04.508+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:04.508+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:04.508+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:04.508+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 ERROR Inbox: Ignoring error
[2024-02-26T16:01:04.508+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.508+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.508+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.508+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.508+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.509+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.509+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.510+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.511+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.512+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.513+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.513+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.513+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.513+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.513+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.513+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.513+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.513+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.514+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.514+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.514+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.514+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.514+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.514+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.514+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.514+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.514+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.514+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.515+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.515+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.515+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.515+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.515+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.515+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.515+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.515+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.515+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.516+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.516+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.517+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.517+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.517+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.517+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.517+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.517+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.517+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.517+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.517+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.517+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.517+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.517+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.518+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.518+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.518+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.518+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.518+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.518+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.518+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.518+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.519+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.519+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.519+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.519+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:04.519+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.519+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.519+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.519+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:04.519+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:04.519+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:04.519+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:04.520+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:04.520+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:04.520+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.520+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:04.520+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:04.520+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:04.520+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:04.520+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:04.520+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.520+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.520+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.521+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.521+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.521+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.521+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.521+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.521+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.521+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.521+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.521+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.521+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.522+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.522+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.522+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.522+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.522+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.523+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.523+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.523+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:04.523+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.524+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.538+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.538+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.538+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.538+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.539+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.539+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.539+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.539+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.539+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.540+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.540+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.540+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.540+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.540+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.540+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.540+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.541+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.541+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.541+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.541+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.541+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.541+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.542+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.554+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.554+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.554+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.554+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.554+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.554+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.554+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.554+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.555+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.555+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.555+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.555+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.555+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.555+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.555+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.555+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.555+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.555+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.555+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.555+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.555+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.555+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.556+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.556+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.556+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.556+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.556+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.556+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.556+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.556+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.556+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.556+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.556+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.557+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.557+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.557+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.557+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.557+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.557+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.557+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:04.557+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:04.557+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:04.557+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 ERROR Inbox: Ignoring error
[2024-02-26T16:01:04.557+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.557+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.557+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.557+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.557+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.558+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.558+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.558+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.559+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.559+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.559+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.559+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.559+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.565+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.565+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.565+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.565+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.565+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.565+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.566+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.566+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.566+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.566+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.566+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.566+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.566+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.566+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.566+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.566+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.566+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.566+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.569+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.569+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.569+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.569+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.569+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.569+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.569+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.569+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.569+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.569+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.569+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.569+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.569+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.570+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.570+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.570+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.570+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.570+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.570+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.570+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.574+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.574+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.574+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.574+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.582+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.582+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.582+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.582+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.582+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.582+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.582+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.582+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.582+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.583+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.583+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.583+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.583+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.583+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.583+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.583+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.584+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.584+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.584+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:04.584+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.584+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.584+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.587+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:04.588+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:04.591+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:04.591+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:04.592+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:04.592+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:04.592+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.595+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:04.596+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:04.596+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:04.596+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:04.597+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:04.597+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.598+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.598+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.598+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.598+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.598+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.599+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.599+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.599+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.599+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.600+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.600+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.600+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.602+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.602+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.602+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.602+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.602+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.602+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.602+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.603+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:04.603+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.604+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.604+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.604+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.608+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.608+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.608+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.609+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.609+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.610+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.610+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.610+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.610+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.623+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.624+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.624+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.624+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.624+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.624+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.624+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.624+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.625+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.625+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.625+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.625+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.625+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.625+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.625+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.625+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.626+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.626+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.626+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.626+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.626+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.626+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.627+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.627+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.627+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.627+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.627+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.627+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.627+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.627+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.628+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.628+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.628+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.628+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.628+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.628+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.628+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.628+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.628+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.629+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.629+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.629+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.629+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.629+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.629+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.629+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.630+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.630+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.630+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.630+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.631+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.632+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:04.632+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:04.632+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:04.632+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 ERROR Inbox: Ignoring error
[2024-02-26T16:01:04.633+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.633+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.633+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.633+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.634+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.634+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.635+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.635+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.635+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.635+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.636+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.636+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.636+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.636+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.636+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.637+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.637+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.637+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.637+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.637+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.637+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.638+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.638+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.639+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.639+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.640+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.640+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.640+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.640+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.640+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-02-26T16:01:04.640+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-02-26T16:01:04.640+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-02-26T16:01:04.641+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-02-26T16:01:04.641+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-02-26T16:01:04.641+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-02-26T16:01:04.641+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-02-26T16:01:04.642+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-02-26T16:01:04.642+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-02-26T16:01:04.642+0000] {spark_submit.py:490} INFO - ... 17 more
[2024-02-26T16:01:04.643+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:04.643+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.643+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.643+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.644+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:04.644+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:04.644+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:04.644+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:04.645+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:04.645+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:04.645+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.645+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:04.645+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:04.645+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:04.646+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:04.646+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:04.646+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.646+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.647+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.647+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.647+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.648+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.648+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.648+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.649+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.649+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.649+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.649+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.650+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.650+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.650+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.650+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.651+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.651+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.652+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.652+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.663+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:04.664+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.665+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.665+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.665+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.666+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.666+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.666+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.666+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.667+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.667+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-02-26T16:01:04.667+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-02-26T16:01:04.668+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-02-26T16:01:04.668+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-02-26T16:01:04.668+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-02-26T16:01:04.668+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-02-26T16:01:04.668+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-02-26T16:01:04.669+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-02-26T16:01:04.669+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-02-26T16:01:04.670+0000] {spark_submit.py:490} INFO - ... 17 more
[2024-02-26T16:01:04.670+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:04.671+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:04.671+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:04.671+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:04.671+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.672+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.672+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.672+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:04.672+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:04.672+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:04.673+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:04.673+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:04.673+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:04.673+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.673+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:04.673+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:04.674+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:04.674+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:04.674+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:04.674+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.674+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.674+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.674+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.675+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.675+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.675+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.675+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.675+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.676+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.676+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.676+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.676+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.676+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.676+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.677+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.678+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.678+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.678+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.678+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.678+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:04.678+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.678+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.678+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.678+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.680+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.680+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.680+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.681+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.681+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.681+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.681+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.681+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.681+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.681+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.681+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.681+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.681+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.682+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.682+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.682+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.682+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.682+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.682+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.682+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.682+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.682+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.683+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.683+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.683+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.683+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.686+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.686+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.686+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.686+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.687+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.687+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.687+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.687+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.687+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.687+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.687+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.687+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.687+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.688+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.688+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.688+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.688+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.688+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.688+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.688+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.688+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.689+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.689+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.689+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.689+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.689+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.689+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.689+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.690+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.690+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.690+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.690+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.690+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.690+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.691+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 ERROR Inbox: Ignoring error
[2024-02-26T16:01:04.691+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.691+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.691+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.691+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.691+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.691+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.691+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.691+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.691+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.692+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.692+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.692+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.692+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.692+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.692+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.693+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.698+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.698+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.699+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.700+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.700+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.700+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.700+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.700+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.701+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.701+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.701+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.701+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.701+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.701+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.702+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.707+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.707+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.707+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.708+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.708+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.708+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.708+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.709+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.709+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.709+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.709+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.709+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.709+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.709+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.710+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.710+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.710+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.710+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.710+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.710+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.711+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.711+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.711+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.711+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.711+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.711+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.711+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.712+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.712+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.712+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.712+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.712+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.713+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.713+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.713+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.713+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.713+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.713+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.713+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.713+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.713+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.714+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.714+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.715+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.715+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.715+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.715+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.715+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.715+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.716+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.716+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.716+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.721+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.722+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:04.722+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:04.722+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:04.722+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 ERROR Inbox: Ignoring error
[2024-02-26T16:01:04.722+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.723+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.723+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.724+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.724+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.724+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.724+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.724+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.724+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.724+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.724+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.725+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.725+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.726+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.726+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.726+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.726+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.726+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.727+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.728+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.728+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.728+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.728+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.728+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.728+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.728+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.729+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.729+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.729+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.729+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.729+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.729+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.729+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.730+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.730+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.730+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.730+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.730+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.735+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.735+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.735+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.735+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.735+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.735+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.735+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.735+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.736+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.736+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.736+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.737+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.743+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.744+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.744+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.744+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.744+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.744+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.744+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.744+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.744+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.744+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.744+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.745+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.745+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.745+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.745+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.745+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.745+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.745+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.745+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.745+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.745+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.746+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.746+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.747+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.747+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.747+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.747+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.747+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.747+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.748+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.748+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.748+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.748+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.749+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.749+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:04.749+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.750+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.750+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.750+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:04.750+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:04.750+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:04.751+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:04.755+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:04.755+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:04.755+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.756+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:04.756+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:04.756+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:04.756+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:04.756+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:04.757+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.757+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.757+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.757+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.757+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.758+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.758+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.758+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.758+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.759+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.759+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.760+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.760+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.762+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.763+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.763+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.763+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.764+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.764+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.764+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.765+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:04.765+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.765+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.765+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.766+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.768+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.768+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.768+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.769+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.769+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.769+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.769+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.770+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.770+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.770+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.770+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.771+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.771+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.771+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.771+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.772+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.774+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.774+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.774+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.774+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.775+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.776+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.777+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.777+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.777+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.778+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.778+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.778+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.778+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.779+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.779+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.780+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.782+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.782+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.782+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.783+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.783+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.784+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.784+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.784+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.784+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.784+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.784+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.785+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.785+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.786+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.786+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.786+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.786+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.786+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.787+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.788+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.788+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.789+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.789+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.789+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.789+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.790+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.790+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.790+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.791+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:04.791+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:04.792+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:04.792+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 ERROR Inbox: Ignoring error
[2024-02-26T16:01:04.792+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.792+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.792+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.792+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.792+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.792+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.793+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.793+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.793+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.793+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.793+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.794+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.803+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.803+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.804+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.804+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.804+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.806+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.806+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.806+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.807+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.807+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.807+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.808+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.808+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.808+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.809+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.809+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.809+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.811+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.811+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.811+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.811+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.811+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.811+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.813+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.813+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.813+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.813+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.813+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.813+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.813+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.814+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.814+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.814+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.814+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.814+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.814+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.814+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.814+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.814+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.815+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.815+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.815+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.815+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.815+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.816+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.816+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.816+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.816+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.816+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.816+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.816+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.816+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.817+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.817+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.817+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.817+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.817+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.817+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.818+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.818+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.818+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.818+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.818+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.818+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.818+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.818+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.818+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.819+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.819+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.819+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.819+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.819+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.819+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:04.819+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.820+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.820+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.825+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:04.826+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:04.826+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:04.826+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:04.826+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:04.827+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:04.827+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.828+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:04.828+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:04.828+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:04.829+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:04.829+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:04.829+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.829+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.829+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.829+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.830+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.830+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.830+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.830+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.830+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.830+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.831+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.831+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.831+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.831+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.831+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.832+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.832+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.832+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.832+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.832+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.833+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:04.833+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.833+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.833+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.833+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.833+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.833+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.834+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.834+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.834+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.834+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.834+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.834+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.835+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.835+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.835+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.835+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.835+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.836+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.836+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.836+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.836+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.837+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.837+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.837+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.837+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.837+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.838+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.838+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.838+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.839+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.839+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.839+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.839+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.839+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.840+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.840+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.840+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.840+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.841+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.841+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.841+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.841+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.845+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.846+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.846+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.846+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.846+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.846+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.846+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.846+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.846+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.847+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.847+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.848+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.848+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.848+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.848+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.848+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.848+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.848+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.848+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.849+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.849+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.849+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.849+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:04.850+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:04.850+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:04.850+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 ERROR Inbox: Ignoring error
[2024-02-26T16:01:04.850+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.850+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.850+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.850+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.852+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.852+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.852+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.852+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.852+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.853+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.853+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.853+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.853+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.853+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.854+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.854+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.854+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.854+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.854+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.855+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.855+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.855+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.856+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.856+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.856+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.856+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.856+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.856+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.856+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.857+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.857+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.857+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.858+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.858+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.858+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.859+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.859+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.859+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.864+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.864+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.864+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.865+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.865+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.865+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.865+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.866+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.866+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.867+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.867+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.867+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.867+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.867+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.868+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.868+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.868+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.869+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.869+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.869+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.870+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.870+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.870+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.870+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.871+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.872+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.872+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.879+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.879+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.879+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.879+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.879+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.879+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.879+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.880+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.880+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.880+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.880+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.880+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.880+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.881+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.881+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.881+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.881+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.881+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.881+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.882+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:04.883+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.883+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.883+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.883+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:04.883+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:04.884+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:04.884+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:04.884+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:04.884+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:04.885+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.885+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:04.885+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:04.885+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:04.885+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:04.886+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:04.887+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.887+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.887+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.887+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.887+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.887+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.887+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.887+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.887+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.888+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.888+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.888+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.888+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.891+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.899+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.899+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.899+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.899+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.900+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.902+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.902+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:04.902+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.902+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.927+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.928+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.928+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.928+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.928+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.929+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.929+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.929+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.929+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.929+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.932+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.932+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.932+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.932+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.933+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.933+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.933+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.933+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.933+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.933+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.933+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.933+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.934+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.934+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.935+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.935+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.936+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.936+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.936+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.936+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.936+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.936+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.937+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.937+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.937+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.937+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.938+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.938+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.938+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.938+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.938+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.939+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.939+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.940+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.940+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.940+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.940+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.940+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.941+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.941+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.941+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.942+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.942+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.942+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.942+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.942+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.943+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.943+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.943+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.944+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.944+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.944+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.944+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:04.945+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:04.945+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:04.945+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:04.945+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.946+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.946+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.946+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:04.946+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:04.947+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:04.947+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:04.947+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:04.947+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:04.948+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.948+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:04.948+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:04.948+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:04.948+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:04.948+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:04.949+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.949+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.949+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.950+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.950+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.950+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.950+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.953+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.953+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.953+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.954+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.954+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.954+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.954+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.954+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.954+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.954+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.954+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.955+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.955+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.955+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:04.956+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.956+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.956+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:04.956+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:04.957+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:04.957+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.957+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.957+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.958+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.958+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.959+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.962+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.965+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.966+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.966+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.966+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.966+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.966+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.966+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.967+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.967+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.967+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.967+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.967+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.967+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.967+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.967+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:04.968+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.968+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:04.968+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:04.968+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:04.972+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:04.973+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:04.973+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:04.973+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:04.973+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:04.973+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:04.974+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.974+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.974+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.974+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.975+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:04.975+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:04.975+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:04.976+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:04.976+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:04.976+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:04.976+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:04.976+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:04.977+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:04.977+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:04.977+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:04.977+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:04.977+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:04.978+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:04.978+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:04.978+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:04.978+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:04.978+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:04.979+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:04.979+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:04.979+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:04.982+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:04.982+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:04.982+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 ERROR Inbox: Ignoring error
[2024-02-26T16:01:04.982+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:04.982+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:04.984+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:04.986+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:04.986+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:04.986+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:04.987+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:04.987+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:04.987+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:04.987+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:04.987+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:04.987+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:04.988+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:04.988+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:04.989+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:04.991+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:04.992+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:04.992+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:04.992+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:04.993+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:04.993+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:04.997+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:04.997+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:05.011+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:05.012+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:05.012+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:05.013+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:05.013+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:05.013+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:05.013+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:05.014+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:05.014+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:05.025+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:05.030+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:05.030+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:05.030+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:05.031+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:05.031+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:05.031+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:05.035+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:05.036+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:05.036+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:05.037+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:05.037+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:05.037+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:05.042+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:05.042+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:05.042+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:05.043+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:05.043+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:05.043+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:05.043+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:05.043+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:05.043+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:05.043+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:05.043+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:05.043+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:05.043+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:05.044+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:05.044+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:05.044+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:05.044+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:05.044+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:05.044+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:05.045+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:05.045+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:05.045+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:05.045+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:05.045+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:05.045+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:05.046+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:05.046+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:05.046+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:05.046+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:05.046+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:05.047+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:05.047+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:05.047+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:05.047+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:05.048+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:05.048+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:05.048+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:05.048+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:05.085+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:05.085+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:05.106+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:05.107+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:05.107+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:05.107+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:05.107+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:05.107+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:05.108+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:05.108+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:05.108+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:05.108+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:05.108+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:05.108+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:05.108+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:05.108+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:05.108+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:05.108+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:05.108+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:05.109+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:05.109+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:05.109+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:05.109+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:05.109+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:05.109+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:05.109+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:05.109+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:05.109+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:05.109+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:05.109+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:05.109+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:05.109+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:05.110+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:05.110+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:05.110+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:05.110+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:05.110+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:05.110+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:05.110+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:05.110+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:05.110+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:05.110+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:05.110+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-02-26T16:01:05.111+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-02-26T16:01:05.112+0000] {spark_submit.py:490} INFO - ... 17 more
[2024-02-26T16:01:05.112+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 ERROR Inbox: Ignoring error
[2024-02-26T16:01:05.112+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:05.112+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:05.112+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:05.112+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:05.112+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:05.112+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:05.112+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:05.115+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:05.116+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:05.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:05.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:05.117+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:05.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:05.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:05.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)
[2024-02-26T16:01:05.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)
[2024-02-26T16:01:05.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith(Promise.scala:40)
[2024-02-26T16:01:05.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)
[2024-02-26T16:01:05.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)
[2024-02-26T16:01:05.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap(Future.scala:306)
[2024-02-26T16:01:05.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.flatMap$(Future.scala:306)
[2024-02-26T16:01:05.117+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)
[2024-02-26T16:01:05.117+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)
[2024-02-26T16:01:05.117+0000] {spark_submit.py:490} INFO - ... 17 more
[2024-02-26T16:01:05.121+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO Executor: Told to re-register on heartbeat
[2024-02-26T16:01:05.121+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManager: BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None) re-registering with master
[2024-02-26T16:01:05.121+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1db92e94aa46, 38489, None)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 ERROR Inbox: Ignoring error
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:05.122+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:05.123+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:05.124+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:05.125+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:05.125+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:05.125+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:05.125+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:05.125+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:05.125+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:05.125+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:05.125+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:05.125+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:05.125+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:05.128+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:05.128+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:05.128+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:05.129+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:05.129+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:05.129+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:05.129+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:05.129+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:05.129+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:05.129+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:05.129+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:05.129+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:05.129+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:05.129+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:05.129+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:05.131+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:05.131+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:05.132+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:05.132+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2089 bytes result sent to driver
[2024-02-26T16:01:05.132+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 WARN Executor: Issue communicating with driver in heartbeater
[2024-02-26T16:01:05.132+0000] {spark_submit.py:490} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:05.132+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:05.132+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:05.132+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
[2024-02-26T16:01:05.132+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)
[2024-02-26T16:01:05.132+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)
[2024-02-26T16:01:05.132+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:641)
[2024-02-26T16:01:05.132+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1111)
[2024-02-26T16:01:05.132+0000] {spark_submit.py:490} INFO - at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
[2024-02-26T16:01:05.132+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:05.132+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
[2024-02-26T16:01:05.132+0000] {spark_submit.py:490} INFO - at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
[2024-02-26T16:01:05.133+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
[2024-02-26T16:01:05.133+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
[2024-02-26T16:01:05.133+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
[2024-02-26T16:01:05.133+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2024-02-26T16:01:05.133+0000] {spark_submit.py:490} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2024-02-26T16:01:05.133+0000] {spark_submit.py:490} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2024-02-26T16:01:05.133+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
[2024-02-26T16:01:05.134+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)
[2024-02-26T16:01:05.134+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:117)
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:116)
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:611)
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:610)
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:648)
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:123)
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - ... 3 more
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@1db92e94aa46:40361
[2024-02-26T16:01:05.135+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
[2024-02-26T16:01:05.137+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
[2024-02-26T16:01:05.137+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
[2024-02-26T16:01:05.137+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
[2024-02-26T16:01:05.137+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:05.137+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:05.137+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:05.137+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:05.137+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:05.137+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:05.138+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:05.138+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:05.138+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:05.138+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:05.138+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:05.138+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:05.139+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:05.139+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:05.139+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:05.139+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:05.140+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:05.140+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:05.140+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:05.140+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:05.140+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:05.140+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
[2024-02-26T16:01:05.140+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:05.140+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)
[2024-02-26T16:01:05.141+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)
[2024-02-26T16:01:05.141+0000] {spark_submit.py:490} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2024-02-26T16:01:05.141+0000] {spark_submit.py:490} INFO - at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)
[2024-02-26T16:01:05.141+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)
[2024-02-26T16:01:05.141+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)
[2024-02-26T16:01:05.141+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)
[2024-02-26T16:01:05.141+0000] {spark_submit.py:490} INFO - at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)
[2024-02-26T16:01:05.142+0000] {spark_submit.py:490} INFO - at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
[2024-02-26T16:01:05.142+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:05.142+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:05.142+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:05.142+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:05.143+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess(Promise.scala:94)
[2024-02-26T16:01:05.143+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
[2024-02-26T16:01:05.143+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
[2024-02-26T16:01:05.143+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
[2024-02-26T16:01:05.143+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)
[2024-02-26T16:01:05.143+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)
[2024-02-26T16:01:05.143+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
[2024-02-26T16:01:05.144+0000] {spark_submit.py:490} INFO - at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
[2024-02-26T16:01:05.144+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
[2024-02-26T16:01:05.144+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
[2024-02-26T16:01:05.144+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
[2024-02-26T16:01:05.144+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
[2024-02-26T16:01:05.144+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
[2024-02-26T16:01:05.145+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete(Promise.scala:53)
[2024-02-26T16:01:05.145+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.complete$(Promise.scala:52)
[2024-02-26T16:01:05.145+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)
[2024-02-26T16:01:05.146+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success(Promise.scala:86)
[2024-02-26T16:01:05.146+0000] {spark_submit.py:490} INFO - at scala.concurrent.Promise.success$(Promise.scala:86)
[2024-02-26T16:01:05.146+0000] {spark_submit.py:490} INFO - at scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)
[2024-02-26T16:01:05.146+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)
[2024-02-26T16:01:05.146+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)
[2024-02-26T16:01:05.146+0000] {spark_submit.py:490} INFO - at org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)
[2024-02-26T16:01:05.146+0000] {spark_submit.py:490} INFO - ... 8 more
[2024-02-26T16:01:05.147+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 339626 ms on 1db92e94aa46 (executor driver) (1/2)
[2024-02-26T16:01:05.147+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO PythonRunner: Times: total = 337909, boot = 337669, init = 240, finish = 0
[2024-02-26T16:01:05.147+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 60121
[2024-02-26T16:01:05.147+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2966 bytes result sent to driver
[2024-02-26T16:01:05.147+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 339532 ms on 1db92e94aa46 (executor driver) (2/2)
[2024-02-26T16:01:05.147+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-02-26T16:01:05.147+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:05 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 340.869 s
[2024-02-26T16:01:05.148+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-02-26T16:01:05.148+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-02-26T16:01:05.165+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:05 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 341.050161 s
[2024-02-26T16:01:05.882+0000] {spark_submit.py:490} INFO - 24/02/26 16:01:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 1db92e94aa46:38489 in memory (size: 9.6 KiB, free: 434.4 MiB)
[2024-02-26T16:01:06.207+0000] {spark_submit.py:490} INFO - root
[2024-02-26T16:01:06.215+0000] {spark_submit.py:490} INFO - |-- data: array (nullable = true)
[2024-02-26T16:01:06.215+0000] {spark_submit.py:490} INFO - |    |-- element: struct (containsNull = true)
[2024-02-26T16:01:06.216+0000] {spark_submit.py:490} INFO - |    |    |-- date: string (nullable = true)
[2024-02-26T16:01:06.216+0000] {spark_submit.py:490} INFO - |    |    |-- nav: string (nullable = true)
[2024-02-26T16:01:06.217+0000] {spark_submit.py:490} INFO - |-- meta: struct (nullable = true)
[2024-02-26T16:01:06.222+0000] {spark_submit.py:490} INFO - |    |-- fund_house: string (nullable = true)
[2024-02-26T16:01:06.222+0000] {spark_submit.py:490} INFO - |    |-- scheme_category: string (nullable = true)
[2024-02-26T16:01:06.222+0000] {spark_submit.py:490} INFO - |    |-- scheme_code: long (nullable = true)
[2024-02-26T16:01:06.222+0000] {spark_submit.py:490} INFO - |    |-- scheme_name: string (nullable = true)
[2024-02-26T16:01:06.222+0000] {spark_submit.py:490} INFO - |    |-- scheme_type: string (nullable = true)
[2024-02-26T16:01:06.222+0000] {spark_submit.py:490} INFO - |-- status: string (nullable = true)
[2024-02-26T16:01:06.222+0000] {spark_submit.py:490} INFO - 
[2024-02-26T16:02:02.011+0000] {local_task_job_runner.py:292} WARNING - State of this instance has been externally set to queued. Terminating instance.
[2024-02-26T16:02:02.400+0000] {process_utils.py:135} INFO - Sending Signals.SIGTERM to group 11093. PIDs of all processes in the group: [11093]
[2024-02-26T16:02:02.413+0000] {process_utils.py:86} INFO - Sending the signal Signals.SIGTERM to group 11093
[2024-02-26T16:02:02.486+0000] {taskinstance.py:1517} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-02-26T16:02:02.491+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 18 for task spark_submit_task (Task received SIGTERM signal; 11093)
[2024-02-26T16:02:02.651+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=11093, status='terminated', exitcode=1, started='15:55:01') (11093) terminated with exit code 1
